= Structuration de l’intelligence artificielle par l’ingénierie logicielle : Apports des design patterns à l’intégration des LLM
:page-navtitle: Structuration de l’intelligence artificielle par l’ingénierie logicielle : Apports des design patterns à l’intégration des LLM
:page-excerpt: Explorons l’apport des design patterns à l’intégration des modèles d’IA pour des systèmes intelligents.
:layout: post
:author: rickenbazolo
:page-tags: [Java, Ingénierie logicielle, Design Patterns, Intelligence Artificielle appliquée, Architecture logicielle, Programmation et bonnes pratiques]
:docinfo: shared-footer
:page-vignette: ingenieurie_logicielle_ia.png
:page-vignette-licence: 'Image générée par l'IA'
:page-liquid:
:showtitle:
:page-categories: software llm

L’intelligence artificielle est désormais bien plus qu’une tendance, elle s’intègre au cœur des systèmes logiciels. Mais cette intégration ne se fait pas sans poser des défis techniques majeurs. Comment rendre un système capable de s’adapter à des composants évolutifs, parfois imprévisibles, comme les modèles d’IA. C’est là que l’ingénierie logicielle entre en jeu.

Dans cet article, nous allons explorer deux patterns design, en particulier *Strategy* et *Observer*, pour structurer proprement l’intégration des modèles d’intelligence artificielle dans vos systèmes.

== Intégrer des LLM dans des applications métier, un défi sous-estimé

Les grands modèles de langage (LLM) tels que ceux https://platform.openai.com/docs/models/[d'OpenAI^], https://www.anthropic.com/[Anthropic^] et https://docs.mistral.ai/getting-started/models/models_overview/[Mistral^] pour n'en citer que quelques uns, ouvrent la voie à une nouvelle génération d'applications intelligentes, les outils d'analyse de texte, des générateurs de rapports et des solutions d'automatisation des tâches récurrentes, entre autres.

Mais entre la preuve de concept et une intégration propre dans un système d’information, le fossé est immense.

**Pourquoi ?** Parce qu’un LLM n’est pas un simple service qu’on appelle. C’est une entité complexe, avec :

* Des coups varriables;
* Des comportements dynamiques;
* Des besoins de sécurité, de performance, de contrôle et de monitoring.

Pour les intégrer efficacement dans des applications métier, il faut revenir aux fondamentaux de **l’ingénierie logicielle**.

== Pourquoi l'ingénierie logicielle est essentielle à l’IA métier

L’IA n’est pas une `boîte noire magique` à brancher au bout d’un pipeline. Dans une application métier, elle doit :

* Pouvoir être remplacée;
* Être testée;
* Être surveillée et observée;
* Évoluer en fonction du contexte d'usage.

C’est exactement le rôle de l’ingénierie logicielle, **construire des architectures modifiables**, **robustes et durables**, en appliquant des principes de conception et des patterns éprouvés.

L'ingénierie logicielle permet doc de : 

* Découpler la logique métier de l'intéligence artificielle;
* Encapsuler les modèles dans des abstractions claires;
* Orchestrer leur cycle de vie et leur comportement.

== L’ingénierie logicielle au service de l’IA générative

L’ingénierie logicielle permet de structurer l’intégration d’un LLM en respectant des principes comme :

* **La séparation des responsabilités**;
* **La modularité**;
* **La testabilité**;
* **La réversibilité des choix technologiques**.

C'est ici que les **design patterns** interviennent. En tant que solutions éprouvées pour des problèmes courants de conception logicielle, leur application à l'IA permet :

* De rendre les modèles interchangeables;
* De gérer les évenements liés à leur fonctionnement (`function calling`, `tools`, `agents`);
* De garder un système cohérent même si la couche IA change radicalement.

NOTE: Nous nous concentrons sur deux patterns particulièrement efficaces dans ce contexte, **Strategy** pour abstraire les modèles et **Observer** pour orchestrer les événements liés à leur cycle de vie.

== Strategy Pattern, abstraire et choisir dynamiquement ses LLM

Content (abstraire l’utilisation de LLM selon le contexte métier)

=== Problème

Content

=== Solution

Content

=== Exemple

Content code and use case

== Observer Pattern, orchestrer le cycle de vie des composants IA

Content (Quand un modèle est mis à jour, les systèmes de cache, d’évaluation ou d’interface utilisateur doivent réagir sans couplage direct.)

=== Problème

Content

=== Solution

Content

=== Exemple

Content code and use case

== Structurer l’intégration des LLM dans une architecture métier

Content, bonnes pratiques et exemple d'archi

== Autres patterns utiles : Étendre la structuration IA vers des pipelines

En plus des **Strategy** et **Observer**, d'autres design patterns facilitent une intégration des LLM plus propre, modulaire et alignée avec les besoins métiers. Voici quelques patterns particulièrement pertinents dans ce contexte.

=== Factory Pattern : Instancier dynamiquement des modèles avec des paramètres métier

Lorsque vous devez configurer dynamiquement des appels à un LLM selon le contexte (`créatif`, `concis`, etc.), il est préférable de ne pas exposer ces détails dans tout votre code. Le **Factory Pattern** permet de centraliser cette logique d’instanciation et de garantir la cohérence des configurations.

Exemple:
[source,java]
----
// Définition immuable de la configuration du LLM
public record LLMConfig(String model, double temperature, double topP, int maxTokens)

// Factory centralisant la logique d'instanciation en fonction du contexte
public class LLMFactory {
    public static LLM createLLM(String context) {
         return new LLM(
             switch(context) {
                 case "créatif" -> new LLMConfig("gpt-4", 0.9, 0.95, 150);
                 case "concis"  -> new LLMConfig("gpt-3.5-turbo", 0.5, 0.8, 100);
                 default        -> new LLMConfig("gpt-3.5-turbo", 0.7, 0.9, 120);
             }
         );
    }
}
----

IMPORTANT: Cet exemple permet de centraliser et de modifier facilement la logique de configuration sans avoir à exposer les détails dans tout votre code.

=== Command Pattern - Orchestrer des pipelines IA

Les pipelines IA exécutent une série ordonnée de tâches, telles que `classification` → `résumé` → `génération`.
Le **Command Pattern** peut être utilisé pour encapsuler chaque étape du pipeline dans des objets de commande distincts. Cela permet de gérer les opérations de manière flexible et de les exécuter ou annuler indépendamment.

Exemple :
[source,java]
----
// PipelineContext.java
// Contexte partagé entre les commandes, contenant les données intermédiaires du pipeline.
public class PipelineContext {
    private String input;
    private String classification;
    private String summary;
    private String generation;
    // Implémentation code ..
}

// Command.java
// Interface scellée (sealed) définissant les opérations d'exécution et d'annulation.
public sealed interface Command permits ClassificationCommand, SummarizationCommand, GenerationCommand {
    void execute(PipelineContext context);
    void undo(PipelineContext context);
}

// ClassificationCommand.java
// Commande pour réaliser l'étape de classification.
public final class ClassificationCommand implements Command {
    @Override
    public void execute(PipelineContext context) {
        // Simulation d'un appel à un LLM par exemple, déterminer une catégorie pour le texte d'entrée.
        var result = callLlm("classification: " + context.input());
        context.setClassification(result);
    }

    @Override
    public void undo(PipelineContext context) {
        context.setClassification(null); // Annulation de la classification...
    }
}

// SummarizationCommand.java
// Commande pour réaliser l'étape de résumé.
public final class SummarizationCommand implements Command {
    @Override
    public void execute(PipelineContext context) {
        // Simulation d'un appel à un LLM pour la création d'un résumé basé sur la classification.
        var result = callLlm("summarize: " + context.getClassification());
        context.setSummary(result);
    }

    @Override
    public void undo(PipelineContext context) {
        context.setSummary(null); // Annulation du résumé...
    }
}

// GenerationCommand.java
// Commande pour réaliser l'étape de génération.
public final class GenerationCommand implements Command {
    @Override
    public void execute(PipelineContext context) {
        // Simulation d'un appel à un LLM pour la génération de texte en se basant sur le résumé.
        var result = callLlm("generate: " + context.getSummary());
        context.setGeneration(result);
    }

    @Override
    public void undo(PipelineContext context) {
        context.setGeneration(null); // Annulation de la génération...
    }
}

// Pipeline.java
// Classe orchestrant l'exécution séquentielle des commandes du pipeline.
public class Pipeline {
    private final List<Command> commands;

    public Pipeline(List<Command> commands) {
        this.commands = commands;
    }

    public void execute(PipelineContext context) {
        for (Command command : commands) {
            command.execute(context);
        }
    }

    public void undo(PipelineContext context) {
        // On annule dans l'ordre inverse
        for (int i = commands.size() - 1; i >= 0; i--) {
            commands.get(i).undo(context);
        }
    }
}

// Main.java
// Exemple d'utilisation du pipeline IA avec le Command Pattern.
public class Main {
    public static void main(String[] args) {
        // Création du contexte avec le texte d'entrée
        PipelineContext context = new PipelineContext("Texte d'entrée pour le pipeline IA.");

        // Instanciation des commandes correspondant aux étapes du pipeline
        List<Command> commands = List.of(
            new ClassificationCommand(),
            new SummarizationCommand(),
            new GenerationCommand()
        );

        // Création et exécution du pipeline
        Pipeline pipeline = new Pipeline(commands);
        pipeline.execute(context);

        // Affichage du résultat final
        System.out.println("=== Résultat final du Pipeline ===");
        System.out.println("Classification : " + context.getClassification());
        System.out.println("Résumé         : " + context.getSummary());
        System.out.println("Génération     : " + context.getGeneration());

        // annulation du pipeline (si besoin d'un rollback)
        // pipeline.undo(context);
    }
}
----

NOTE: Cet exemple montre comment le **Command Pattern** peut rendre la gestion d'un pipeline IA flexible, en isolant chaque opération dans un objet distinct et en permettant de les exécuter ou annuler indépendamment,
nous aussi définir des pipelines de type RAG naïve, modulaire, etc.

== Conclusion

Dans cet article, nous avons exploré quelques design patterns applicables au domaine de l'IA, mais il en existe bien d'autres à découvrir. Par exemple, le **Decorator Pattern** peut être utilisé pour ajouter dynamiquement des responsabilités supplémentaires à des objets dans un système **RAG (Retrieval Augmented Generation)**, permettant ainsi de tester, remplacer ou surveiller chaque étape du processus de génération augmentée par récupération.

L'intégration de LLM dans les applications métier ne se résume pas à la simple connexion d'une API ou à du **prompt engineering**. Elle nécessite la conception d'une **architecture intelligente**, fondée sur des abstractions et des interfaces solides, capables de s'adapter aux évolutions, de répondre aux divers contextes d'utilisation, et d'être testées, maintenues et évolutives sur le long terme.





