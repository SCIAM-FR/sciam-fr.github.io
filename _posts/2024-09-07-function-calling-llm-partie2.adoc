= Du Concept à l'Action : Le Function Calling des LLM et son Utilisation avec Spring AI - Partie 2
:page-navtitle:Du Concept à l'Action : Le Function Calling des LLM et son Utilisation avec Spring AI - Partie 2
:page-excerpt: Ce second article se concentre  sur l'implémentation pratique du Function Calling avec Spring AI dans une application Java
:layout: post
:author: rickenbazolo
:page-tags: [IA, LLM, FunctionCalling, agentAI, tools]
:docinfo: shared-footer
:page-vignette: function_calling_llm_partie1.jpg
:page-vignette-licence: 'Image générée par l'IA'
:page-liquid:
:showtitle:
:page-categories: software llm news

Le Function Calling introduit une capacité clé pour les LLM link:{% post_url 2024-08-28-function-calling-llm-partie1 %}[(Partie 1)^], leur permettant d'interagir directement avec les données et les systèmes d'entreprise. 
Grâce à cette fonctionnalité, les développeurs peuvent créer des applications plus interactives et fonctionnelles. 
En intégrant les LLM aux systèmes et API externes, ceux-ci deviennent de véritables agents IA, capables de réaliser une large gamme de tâches de manière autonome et efficace, transformant ainsi les applications en systèmes intelligents et proactifs.

Ce second article se concentre sur l'implémentation pratique du *Function Calling* avec https://spring.io/projects/spring-ai[Spring AI^] dans une application Spring Boot. 
Nous y expliquerons étape par étape comment configurer et intégrer cette fonctionnalité pour permettre à votre application de tirer pleinement parti des capacités des LLM, en déclenchant des actions concrètes et automatisées en réponse à des interactions utilisateur. 
Cette partie mettra en lumière les aspects techniques pour illustrer l'utilisation du Function Calling dans un environnement Spring.

== L’intégration du Function Calling avec Spring AI

*Spring AI* prend en charge les principaux fournisseurs de modèles tels que https://platform.openai.com/docs/overview[OpenAI^], https://docs.mistral.ai[Mistral AI^], https://www.anthropic.com[Anthropic^] qui ont des versions de LLM qui supportent le Function Calling.

Pour exploiter les capacités des LLM dans votre application Spring Boot, il est nécessaire d'ajouter une ou plusieurs dépendances spécifiques.

== Les dépendances

À la date de rédaction de cet article, le projet Spring AI est actuellement en https://docs.spring.io/spring-ai/reference/1.0/index.html[version 1.0.0-M2^], avec des artefacts disponibles dans les dépôts Spring Milestone et Snapshot.
Pour y accéder, vous devez ajouter les références aux référentiels Spring Milestone dans votre fichier pom.xml.

[source,xml]
----
<repositories>
    <repository>
        <id>spring-milestones</id>
        <name>Spring milestones</name>
        <url>https://repo.spring.io/milestone</url>
    </repository>
</repositories>
----

Maintenant, ajoutons la dépendance pour l’intégration du Function Calling Mistral :

[source,xml]
----
<dependency>
      <groupId>org.springframework.ai</groupId>
      <artifactId>spring-ai-mistral-ai-spring-boot-starter</artifactId>
</dependency>
----

== La configuration de l’application

Spring AI offre une auto-configuration pour l’ensemble de LLM supporté.

Nous avons besoin d’une clé API que vous pouvez créer sur la https://console.mistral.ai/api-keys[console Mistral^] pour utiliser le LLM avec Spring AI.

[source,yml]
----
spring:
  ai:
    mistralai:
      api-key: ${MISTRAL_AI_API_KEY} <1>
      chat:
        options:
          model: mistral-small-latest <2>
----

** `1` Configuration de la clé API pour utiliser le modèle Mistral 
** `2` Définition du nom du modèle à utiliser

NOTE: Actuellement, il n'est pas possible d'utiliser plusieurs modèles avec l'auto-configuration dans une même application. Si vous souhaitez utiliser la configuration par défaut, vous devez spécifier lequel activer en désactivant les autres via les propriétés appropriées, par exemple en mettant `spring.ai.mistralai.chat.enabled=false` ou `spring.ai.azure.openai.chat.enabled=false` si vous avez configuré à la fois Mistral et Azure OpenAI dans votre application.

Cette configuration de base permet à notre application d'utiliser le Function Calling de Mistral avec le modèle *mistral-small-latest*.

NOTE: Actuellement pour Mistral, le Function Calling est disponible pour les modèles suivants : `Mistral Small`; `Mistral Large`; `Mixstral 8x22B`; `Mistral Nemo`

Dans notre cas d'utilisation, nous allons développer une fonction qui renvoie le statut d'une commande en utilisant son numéro de commande.

== La mise en place du ChatClient

Le https://docs.spring.io/spring-ai/reference/1.0/api/chatclient.html[ChatClient^] fournit une API pour interagir avec un LLM, prenant en charge à la fois des programmations synchrones et réactives. 
Cette API permet de créer des invites, constituées de messages, qui sont envoyées au modèle pour guider sa réponse et son comportement.

Nous allons mettre en place un contrôleur REST pour configurer notre *ChatClient*.

[source,java]
----
@RestController
public class DemoController {

    private final ChatClient chatClient;

    public DemoController(ChatClient.Builder chatClient) {
        this.chatClient = chatClient.build();
    }
}
----

Ensuite, nous allons définir un message système par défaut pour attribuer un rôle à notre IA.

[source,java]
----
public DemoController(ChatClient.Builder chatClient) {
        this.chatClient = chatClient
                .defaultSystem("Vous êtes un assistant qui donne l'état des commandes en fonction du numéro de commande. ")
                .build();
}
----

Pour tester notre client de chat, nous allons créer une méthode *askOrderStatus* qui recevra un numéro de commande en paramètre et utilisera le ChatClient pour construire l'invite à envoyer au LLM.

[source,java]
----
@GetMapping(value = "/statut")
public String askOrderStatus(String numeroCommande) {
    return chatClient.prompt()
            .user("Quel est le statut de la commande " + numeroCommande + "?")
            .call()
            .content();
}
----

Après avoir démarré notre application, testons notre API en accédant à ce lien :

http://localhost:8080/statut?numeroCommande=0010


== Conclusion

Le Function Calling relie les LLM (Large Language Models) au concept des agents IA en leur permettant de devenir plus autonomes et actifs dans les systèmes.
Grâce à cette fonctionnalité, les LLM ne se contentent plus de comprendre et répondre les demandes des utilisateurs, mais peuvent aussi exécuter des actions en appelant des fonctions spécifiques.
Cela les transforme en véritables agents intelligents, capables de gérer des tâches complexes, de s'adapter aux contextes variés, et de mieux interagir avec les systèmes informatiques.
Cette autonomie, tout en restant sous le contrôle des développeurs, fait des LLM avec Function Calling un outil puissant pour créer des agents IA avancés.
Dans le second article, nous explorerons comment intégrer le Function Calling dans une application Java avec Spring AI.
