= Model Context Protocol (MCP) : une interface cognitive, pas une API
:page-navtitle: Model Context Protocol (MCP) : une interface cognitive, pas une API
:page-excerpt: Dans le contexte de l'intégration des Grands modèles de langage dans les applications d'entreprise, le choix de l'interface d'exposition des services devient une question architecturale déterminante. Deux paradigmes se distinguent par leurs philosophies radicalement différentes.
:layout: post
:author: rickenbazolo
:page-tags: [LLM, MCP, API, OpenAPI, Ingénierie logicielle, Architecture logicielle]
:docinfo: shared-footer
:page-vignette: api-vs-mcp.png
:page-vignette-licence: 'Image générée par l'IA'
:page-liquid:
:showtitle:
:page-categories: software llm

Dans le contexte de l'intégration des Grands modèles de langage (*LLM*) dans les applications d'entreprise, le choix de l'interface d'exposition des services devient une question architecturale déterminante.
Deux paradigmes se distinguent par leurs philosophies radicalement différentes :

- Les **API d'entreprise** (REST, GraphQL, gRPC) : offrent des contrats statiques et une logique transactionnelle éprouvée pour la communication entre systèmes.
- Le **Model Context Protocol** (MCP) : lancé par https://www.anthropic.com/news/model-context-protocol[Anthropic^] en novembre 2024, introduit une découverte dynamique et une interprétation contextuelle spécifiquement conçues pour les LLM.

Au premier abord, un serveur MCP peut ressembler à une API REST classique, il expose des fonctionnalités, accepte des requêtes et retourne des résultats.
Pourtant, cette similarité de surface masque une différence fondamentale, là où une API décrit des ressources et des opérations, un serveur MCP décrit des capacités sémantiques que les modèles peuvent découvrir, comprendre et invoquer selon le contexte d'une conversation.

Face à cette émergence, comment distinguer véritablement ces deux approches et identifier les implications architecturales de ce changement de paradigme ?

Cet article propose une exploration approfondie de ces deux modèles avec des exemples en Java et le MCP Java SDK, pour éclairer leurs différences conceptuelles et architecturales.
Nous analyserons pourquoi https://www.openapis.org/[OpenAPI^] ne peut pas décrire un serveur *MCP*, comment le rôle du développeur évolue de l'intégrateur à l'architecte d'outils, et fournirons des implémentations réelles pour vous aider à comprendre la coexistence de ces paradigmes complémentaires dans l'ingénierie logicielle.

== L'API d'entreprise : un contrat statique pour des échanges déterministes

Dans l'écosystème des systèmes d'information, les API d'entreprise, qu'elles soient REST, GraphQL ou gRPC, reposent sur un principe fondamental : le *contrat explicite*.
Une spécification OpenAPI, un schéma GraphQL ou des fichiers Protocol Buffers décrivent exhaustivement les endpoints, les structures de données et les comportements attendus.

=== Caractéristiques essentielles

Une API d'entreprise est :

*Spécifiée* : le contrat est défini à l'avance, documenté (OpenAPI, https://www.asyncapi.com/[AsyncAPI^]) et versionné.
Les clients connaissent les endpoints avant l'exécution.

*Transactionnelle* : chaque appel produit un effet métier mesurable (création, lecture, modification, suppression d'une ressource).

*Déterministe* : pour une requête donnée, la réponse suit toujours la même structure.
Un `GET /articles?type=TECHNICAL` retourne systématiquement le même format JSON.

*Orientée ressource* : l'architecture REST, par exemple, organise les interactions autour de ressources identifiées par des URI (`/articles`, `/articles/{id}`).

*Stable* : les modifications du contrat nécessitent un versionnement explicite pour préserver la compatibilité avec les clients existants.

Voici un exemple minimaliste d'une API REST en Spring Boot :

[source,java]
----
@RestController
@RequestMapping("/api/articles")
public class ArticleController {

    @GetMapping
    public List<ArticleDto> getArticles(@RequestParam(required = false) ArticleType type) {
        return type != null
            ? articleService.getArticlesByType(type)
            : articleService.getAllArticles();
    }

    @PostMapping
    public ArticleDto createArticle(@Valid @RequestBody ArticleInputDto input) {
        return articleService.createArticle(input);
    }
}
----

Cette approche a fait ses preuves pour l'intégration de systèmes d'information.
Elle garantit cohérence, traçabilité et prévisibilité dans des architectures avec lesquelles les clients (humains ou logiciels) consomment des services selon des conventions partagées.

Mais cette logique n'a jamais été conçue pour des acteurs capables de raisonnement contextuel comme les LLM.

== Le serveur MCP : une interface contextuelle pour les LLM

Le **Model Context Protocol** (MCP), répond à un problème différent, comment permettre aux LLM d'accéder à des outils et des données de manière *contextuelle et dynamique*, sans multiplier les intégrations personnalisées pour chaque combinaison LLM × outil.

=== Une architecture client-serveur repensée pour les LLM

MCP adopte une architecture client-serveur, mais avec une inversion conceptuelle : le *client* est généralement une application d'IA (Claude Desktop, un IDE, un agent), tandis que le *serveur* expose des capacités que le modèle peut découvrir et invoquer.

Le protocole repose sur trois primitives fondamentales côté serveur :

*Tools* : fonctions exécutables que le LLM peut appeler pour effectuer des actions (rechercher dans une base de données, envoyer un e-mail, lire un fichier, etc.)

*Resources* : données structurées que le LLM peut consulter pour enrichir son contexte (documents, états système, logs, etc.)

*Prompts* : templates d'instructions préconfigurés que les utilisateurs peuvent invoquer via l'application cliente.

La communication s'effectue via des messages *JSON-RPC*, transportés soit par stdio (processus locaux), soit par HTTP Streamable (services distants).

Exemple de requêtes envoyées du client au serveur ou vice versa, afin d'initier une opération :

[source,json]
----
{
  "jsonrpc": "2.0",
  "method": "notifications/tools/list_changed",
  "id": 1
}
----

Le serveur répond avec une description sémantique de chaque tool.

Voici un exemple de réponse :

[source,json]
----
{
  "jsonrpc": "2.0",
  "id": 1,
  "result": {
    "tools": [
      {
        "name": "search_articles",
        "description": "Recherche des articles de blog par type ou mot-clé",
        "inputSchema": {
          "type": "object",
          "properties": {
            "query": {
              "type": "string",
              "description": "Mot-clé à rechercher dans les articles"
            },
            "type": {
              "type": "string",
              "enum": ["TECHNICAL", "SCIENTIFIC"],
              "description": "Filtrer par type d'article"
            }
          }
        }
      },
      {
        "name": "create_article",
        "description": "Crée un nouvel article de blog avec le titre et le contenu fournis",
        "inputSchema": {
          "type": "object",
          "properties": {
            "title": { "type": "string" },
            "content": { "type": "string" },
            "type": { "type": "string", "enum": ["TECHNICAL", "SCIENTIFIC"] }
          },
          "required": ["title", "content", "type"]
        }
      }
    ]
  }
}
----

Le modèle analyse ces descriptions en langage naturel et décide, selon le contexte de la conversation, quel tool invoquer.
L'invocation elle-même est un message JSON-RPC :

[source,json]
----
{
  "jsonrpc": "2.0",
  "method": "tools/call",
  "params": {
    "name": "search_articles",
    "arguments": {
      "query": "architecture hexagonale",
      "type": "TECHNICAL"
    }
  },
  "id": 2
}
----

=== Un protocole conversationnel, pas transactionnel

L'implémentation d'un serveur MCP avec le MCP Java SDK illustre cette différence de paradigme.
Ajoutons d'abord la dépendance Maven :

[source,xml]
----
<dependency>
    <groupId>io.modelcontextprotocol.sdk</groupId>
    <artifactId>mcp</artifactId>
    <version>0.16.0</version>
</dependency>
----

Voici un exemple de serveur MCP synchrone exposant des tools pour gérer des articles :

[source,java]
----
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import io.modelcontextprotocol.json.McpJsonMapper;
import io.modelcontextprotocol.server.McpServer;
import io.modelcontextprotocol.server.McpServerFeatures;
import io.modelcontextprotocol.server.McpSyncServer;
import io.modelcontextprotocol.server.transport.StdioServerTransportProvider;
import io.modelcontextprotocol.spec.McpSchema;

import java.util.*;
import java.util.logging.Logger;

public class BlogMcpServer {

    private static final Logger logger = Logger.getLogger(BlogMcpServer.class.getName());
    private static final ObjectMapper objectMapper = new ObjectMapper();

    public static void main(String[] args) {
        try {
            // 1. Créer le transport (stdio pour communication locale)
            StdioServerTransportProvider transport = new StdioServerTransportProvider(McpJsonMapper.getDefault());

            // 2. Créer le serveur MCP synchrone
            McpSyncServer server = McpServer.sync(transport)
                    .serverInfo("blog-mcp-server", "1.0.0")
                    .capabilities(McpSchema.ServerCapabilities.builder()
                            .tools(true)
                            .resources(false, false)
                            .prompts(false)
                            .build())
                    .build();

            // 3. Définir l'outil de recherche d'articles
            McpServerFeatures.SyncToolSpecification searchTool =
                    new McpServerFeatures.SyncToolSpecification(
                            McpSchema.Tool.builder()
                                    .name("search_articles")
                                    .description("Rechercher des articles de blog par type ou mot-clé")
                                    .inputSchema(McpJsonMapper.getDefault(), """
                                            {
                                              "type": "object",
                                              "properties": {
                                                "query": {
                                                  "type": "string",
                                                  "description": "Mot-clé à rechercher"
                                                },
                                                "type": {
                                                  "type": "string",
                                                  "enum": ["TECHNICAL", "SCIENTIFIC"],
                                                  "description": "Filtrer par type"
                                                }
                                              }
                                            }
                                            """)
                                    .build(),
                            null,
                            (exchange, request) -> {
                                Map<String, Object> arguments = request.arguments();
                                String query = (String) arguments.get("query");
                                String type = (String) arguments.get("type");

                                // Logique métier : recherche dans la base de données
                                List<Article> results = ArticleRepository.search(query, type);

                                String jsonResults;
                                try {
                                    jsonResults = objectMapper.writeValueAsString(results);
                                } catch (JsonProcessingException e) {
                                    logger.severe("Erreur lors de la sérialisation des résultats de recherche : " + e.getMessage());
                                    return McpSchema.CallToolResult.builder()
                                            .addTextContent("Erreur : " + e.getMessage())
                                            .isError(true)
                                            .build();
                                }

                                return McpSchema.CallToolResult.builder()
                                        .addTextContent(jsonResults)
                                        .isError(false)
                                        .build();
                            }
                    );

            // 4. Définir l'outil de création d'article
            McpServerFeatures.SyncToolSpecification createTool =
                    new McpServerFeatures.SyncToolSpecification(
                            McpSchema.Tool.builder()
                                    .name("create_article")
                                    .description("Créer un nouvel article de blog")
                                    .inputSchema(McpJsonMapper.getDefault(), """
                                            {
                                              "type": "object",
                                              "properties": {
                                                "title": { "type": "string" },
                                                "content": { "type": "string" },
                                                "type": {
                                                  "type": "string",
                                                  "enum": ["TECHNICAL", "SCIENTIFIC"]
                                                }
                                              },
                                              "required": ["title", "content", "type"]
                                            }
                                            """)
                                    .build(),
                            null, // paramètre call déprécié
                            (exchange, request) -> {
                                Map<String, Object> arguments = request.arguments();
                                String title = (String) arguments.get("title");
                                String content = (String) arguments.get("content");
                                String type = (String) arguments.get("type");

                                // Logique métier : création dans la base de données
                                Article created = ArticleRepository.create(title, content, type);

                                return McpSchema.CallToolResult.builder()
                                        .addTextContent("Article créé avec succès : " + created.id())
                                        .isError(false)
                                        .build();
                            }
                    );

            // 5. Enregistrer les outils
            server.addTool(searchTool);
            server.addTool(createTool);

            // 6. Le serveur est prêt à recevoir des connexions
            logger.info("Serveur MCP Blog démarré et en attente de connexions");
            System.err.println("Serveur MCP Blog démarré et en attente de connexions");

            // Maintenir le serveur actif - il écoute automatiquement sur stdin/stdout
            Thread.currentThread().join();

        } catch (Exception e) {
            logger.severe("Erreur lors du démarrage du serveur MCP : " + e.getMessage());
            System.exit(1);
        }
    }
}
----

Le serveur ne gère pas d'endpoints HTTP comme `/articles` ou `/articles/{id}`.
Il répond à des messages JSON-RPC décrivant des *intentions* (`tools/list`, `tools/call`), pas des opérations CRUD sur des ressources.
Le modèle raisonne sur les descriptions pour décider quoi appeler et comment construire les arguments.

=== L'absence de contrat figé

Un serveur MCP n'impose pas de schéma global.
Les tools peuvent être ajoutés, modifiés ou retirés sans casser les clients, car *le modèle s'adapte dynamiquement aux capacités disponibles*.
Cette flexibilité est fondamentale : MCP est conçu pour des systèmes où le contexte évolue en temps réel, où les outils disponibles dépendent de l'environnement d'exécution, et où l'interprétation sémantique remplace la validation syntaxique stricte.

*C'est ce décalage conceptuel qui rend impossible de décrire un serveur MCP avec une spécification OpenAPI classique*.

== Pourquoi OpenAPI ne peut pas décrire un serveur MCP

À première vue, on pourrait penser qu'une spécification OpenAPI pourrait documenter un serveur MCP.
Après tout, les deux systèmes échangent des messages structurés et exposent des fonctionnalités.
Pourtant, cette similarité de surface masque des différences architecturales fondamentales qui rendent OpenAPI inadapté pour décrire MCP.

=== Le tableau des divergences

Voici une comparaison systématique des caractéristiques des deux approches :

[cols="1,2,2", options="header"]
|===
| Aspect
| API OpenAPI
| Serveur MCP

| *Contrat*
| Statique, défini à l'avance via spécification YAML/JSON
| Dynamique, découvert à l'exécution via `tools/list`

| *Documentation*
| Spécification OpenAPI exhaustive avant déploiement
| Description sémantique minimale des tools

| *Découverte*
| Endpoints connus avant l'invocation
| Capacités interrogées dynamiquement

| *Typage*
| Schémas JSON rigides avec validation stricte
| Schémas adaptatifs interprétés contextuellement

| *Invocation*
| Requêtes HTTP (GET, POST, PUT, DELETE) sur des URI
| Messages JSON-RPC (`tools/call`) avec arguments

| *Sémantique*
| Orientée ressource (`/articles`, `/articles/{id}`)
| Orientée intention (`search_articles`, `create_article`)

| *Acteur principal*
| Client HTTP (humain ou application)
| Grand Modèle de langage (LLM)

| *Objectif*
| Exposer un service métier transactionnel
| Étendre les capacités cognitives d'un LLM

| *Adaptabilité*
| Changements = nouveau versionnement du contrat
| Tools ajoutés/retirés sans impact client

| *Réponse*
| Structure JSON prédéfinie
| Contenu textuel ou structuré selon contexte

| *Validation*
| Syntaxique stricte (JSON Schema)
| Sémantique par le modèle
|===

=== Le fossé conceptuel : syntaxe vs sémantique

La différence fondamentale réside dans la manière dont les deux systèmes traitent l'information.

*OpenAPI décrit la syntaxe* : une API REST avec OpenAPI spécifie exactement quels endpoints existent, quelles méthodes HTTP sont supportées, quelles structures JSON sont attendues en entrée et en sortie.
Un client REST doit connaître ces détails avant d'invoquer l'API.
Par exemple :

[source,yaml]
----
paths:
  /articles/{id}:
    get:
      summary: Récupère un article par son ID
      parameters:
        - name: id
          in: path
          required: true
          schema:
            type: string
            format: uuid
      responses:
        '200':
          description: Article trouvé
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Article'
        '404':
          description: Article non trouvé
----

Ce contrat est figé, le client sait qu'il doit faire un `GET /articles/{id}` avec un ID valide, et il recevra soit un objet `Article` en JSON, soit une erreur 404.

*MCP décrit la sémantique* : un serveur MCP expose des tools avec des descriptions en langage naturel.
Le modèle interprète ces descriptions pour décider quand et comment utiliser chaque tool.
Par exemple :

[source,json]
----
{
  "name": "get_article",
  "description": "Récupère un article de blog spécifique en utilisant son identifiant unique",
  "inputSchema": {
    "type": "object",
    "properties": {
      "article_id": {
        "type": "string",
        "description": "L'identifiant UUID de l'article à récupérer"
      }
    },
    "required": ["article_id"]
  }
}
----

Le modèle analyse cette description et détermine contextuellement si ce tool est approprié pour répondre à une question de l'utilisateur comme « Montre-moi l'article sur l'architecture hexagonale ».
Le LLM pourrait d'abord chercher l'ID avec `search_articles`, puis invoquer `get_article` avec l'ID trouvé.

=== L'impossibilité de capturer le contexte conversationnel

Une API OpenAPI décrit des opérations *isolées*.
Chaque requête est autonome, sans mémoire du contexte précédent (sauf via des tokens/sessions gérés explicitement).
Le client construit chaque requête indépendamment.

Un serveur MCP, en revanche, fonctionne dans un *contexte conversationnel*.
Bien que les LLM soient intrinsèquement stateless, (ils ne conservent pas de mémoire entre les appels), l'application cliente (le host MCP) maintient l'historique de la conversation et le réinjecte dans le contexte à chaque nouvelle requête.
Cette architecture permet au modèle de comprendre les références implicites et d'enchaîner plusieurs appels de tools pour répondre à une seule question de l'utilisateur, en s'appuyant sur les échanges précédents.

Exemple de conversation :

----
Utilisateur : "Trouve-moi les articles techniques sur Spring"
[Host MCP] → Contexte : [historique vide]
LLM : [appelle search_articles avec query="Spring", type="TECHNICAL"]
LLM : "J'ai trouvé 3 articles : Spring Boot Security, Spring AI, Spring MVC"

Utilisateur : "Montre-moi le premier"
[Host MCP] → Contexte : [historique incluant la recherche précédente et ses résultats]
LLM : [comprend que "le premier" fait référence à "Spring Boot Security"]
LLM : [appelle get_article avec l'ID correspondant]
LLM : "Voici l'article Spring Boot Security : [contenu]"
----

Cette continuité contextuelle, orchestrée par le client MCP qui réinjecte l'historique à chaque tour, n'existe pas dans une API REST classique.
OpenAPI ne peut pas décrire comment un tool s'insère dans un flux conversationnel multi-tours où chaque appel dépend potentiellement des résultats précédents conservés en mémoire par l'application cliente.

=== L'absence de notion de « resource » au sens REST

Dans REST, une ressource est une entité identifiable par une URI (`/articles/123`).
Les opérations CRUD sont standardisées via les verbes HTTP.

Dans MCP, il n'y a pas de notion de ressource REST.
Les *tools* sont des actions arbitraires que le modèle peut invoquer.
Rien n'oblige un serveur MCP à suivre une logique CRUD.
Un tool pourrait être `analyze_sentiment`, `generate_summary` ou `send_notification`, des opérations qui ne correspondent à aucune ressource REST.

Même la primitive *Resources* de MCP (différente des ressources REST) représente des données accessibles via des URI templates, mais elles ne sont pas manipulées via des verbes HTTP.
Elles sont simplement *consultées* pour enrichir le contexte du modèle.

=== Le rôle de l'interprétation humaine (LLM)

OpenAPI est conçu pour être consommé par des développeurs ou des outils automatisés (générateurs de code, validateurs).
La spécification est lue par des humains ou des machines qui génèrent des clients typés.

MCP est conçu pour être *interprété par des LLM*.
Les descriptions des tools sont rédigées en langage naturel précisément pour que le LLM puisse raisonner sur leur utilité.
Aucune génération de code n'est nécessaire : le modèle décide dynamiquement, à chaque conversation, quels tools appeler et dans quel ordre.

Cette différence rend OpenAPI fondamentalement inadapté, une spécification OpenAPI est trop rigide pour capturer l'adaptabilité sémantique que MCP requiert.

== De l'architecture RESTful à l'architecture contextuelle

Le passage des API d'entreprise aux serveurs MCP ne se résume pas à un simple changement de protocole.
Il s'agit d'un *changement de paradigme architectural* qui transforme profondément la manière dont nous concevons les interfaces logicielles.

=== Du client traditionnel au modèle intelligent

Dans une architecture API classique, la relation est claire : un *client* (application web, mobile ou système tiers) envoie des requêtes à un *serveur* qui expose des ressources. Le client connaît les endpoints, construit les requêtes selon le contrat défini et traite les réponses structurées.

[source,java]
----
// Architecture classique : le client pilote explicitement
RestTemplate client = new RestTemplate();
ResponseEntity<List<Article>> response = client.exchange(
    "https://api.blog.com/articles?type=TECHNICAL",
    HttpMethod.GET,
    null,
    new ParameterizedTypeReference<List<Article>>() {}
);
List<Article> articles = response.getBody();
----

Nous écrivons explicitement chaque appel, connaissons la structure des réponses et gérons les erreurs selon les codes HTTP.

Avec MCP, la relation devient : un *LLM* découvre et invoque dynamiquement des *outils* mis à sa disposition.
Le modèle n'a pas de connaissance préalable des endpoints.
Il découvre les capacités disponibles, lit les descriptions et décide contextuellement quoi appeler.

[source,java]
----
// Architecture MCP : le modèle raisonne et décide
ChatClient chatClient = ChatClient.builder()
    .defaultSystem("""
        Tu es un assistant qui aide à rechercher et analyser des articles techniques.
        Utilise les outils à ta disposition pour répondre aux questions.
    """)
    .defaultToolCallbacks(tools)
    .build();

String response = chatClient.prompt()
    .user("Quels sont les articles sur Spring publiés récemment ?")
    .call()
    .content();
----

Le modèle a automatiquement :

1. Compris qu'il devait chercher des articles
2. Invoqué search_articles avec les bons paramètres
3. Analysé les résultats
4. Formulé une réponse naturelle

Nous ne spécifions pas _comment_ obtenir l'information.
Nous fournissons des outils (tools), et le modèle décide de leur utilisation.

=== De la spécification impérative à l'intention déclarative

Dans une API REST, nous devons spécifier *impérativement* les opérations :

[source,java]
----
// 1. Chercher les articles techniques
List<Article> technicalArticles = articleClient.getArticles("TECHNICAL");

// 2. Filtrer par date récente
List<Article> recentArticles = technicalArticles.stream()
    .filter(a -> a.getPublishedDate().isAfter(LocalDate.now().minusMonths(3)))
    .toList();

// 3. Trier par popularité
List<Article> sortedArticles = recentArticles.stream()
    .sorted(Comparator.comparing(Article::getViews).reversed())
    .limit(5)
    .toList();
----

Chaque étape est explicite.
Nous orchestrons manuellement la logique.

Avec MCP, nous exprimons une *intention déclarative* :

[source,java]
----
String response = chatClient.prompt()
    .user("Donne-moi les 5 articles techniques les plus populaires publiés ces 3 derniers mois")
    .call()
    .content();
----

Le modèle interprète l'intention et identifie les outils à utiliser (peut-être `search_articles` avec filtres, ou plusieurs appels successifs).
L'exécution effective de ces tools reste sous le contrôle de l'application cliente (le host MCP), qui décide d'autoriser ou non chaque appel avant de communiquer avec le serveur MCP via le protocole défini.

IMPORTANT: Dans l'architecture MCP, le LLM identifie les tools appropriés et propose leur invocation, mais c'est le client MCP (l'application host) qui a le contrôle final sur leur exécution.
Pour les tools locaux, l'application peut implémenter des mécanismes d'autorisation pour décider si un tool doit être exécuté ou non.
Pour les tools distants, le client gère également l'autorisation de la communication avec les serveurs MCP selon les règles de sécurité définies.
Cette séparation entre identification (par le LLM) et exécution (par l'application) garantit la gouvernance et la sécurité du système.

=== Les implications pour la conception

Ce changement de paradigme a des conséquences directes sur la conception des interfaces.

==== 1. Les descriptions deviennent critiques

Dans une API REST, la documentation est importante, mais le contrat reste le code (les endpoints, les schémas). Nous pouvons comprendre une API mal documentée en lisant le code ou en testant les endpoints.

Dans MCP, les descriptions sont *essentielles* car elles guident directement les décisions du modèle. Une description imprécise ou ambiguë entraîne des invocations incorrectes.

[source,java]
----
// ❌ Mauvaise description
new McpSchema.Tool(
    "search",
    "Cherche des trucs",  // Trop vague
    inputSchema
)

// ✅ Bonne description
new McpSchema.Tool(
    "search_articles",
    "Recherche des articles de blog par mot-clé et/ou type (TECHNICAL/SCIENTIFIC). " +
    "Retourne une liste d'articles correspondant aux critères fournis.",
    inputSchema
);
----

==== 2. Les outils doivent être composables

Les tools MCP sont conçus pour être *combinés* par le modèle.
Un outil ne doit pas essayer de tout faire.
Il vaut mieux exposer plusieurs tools simples que le modèle peut enchaîner.

Exemple Tool monolithique difficile à utiliser :

[source,java]
----
McpSchema.Tool("manage_articles",
    "Crée, recherche, met à jour ou supprime des articles selon l'action spécifiée",
    complexSchema)
----

Exemple Tools granulaires composables :

[source,java]
----
McpSchema.Tool("search_articles", "Recherche des articles", searchSchema)
McpSchema.Tool("get_article", "Récupère un article par ID", getSchema)
McpSchema.Tool("create_article", "Crée un nouvel article", createSchema)
McpSchema.Tool("update_article", "Modifie un article existant", updateSchema)
----

Le modèle peut ainsi composer `search_articles` puis `get_article` pour répondre à « Montre-moi le dernier article sur Spring ».

==== 3. Les erreurs deviennent des informations

Dans une API REST, une erreur 404 ou 500 est souvent finale pour le client.
Il doit gérer l'exception et éventuellement alerter l'utilisateur.

Dans MCP, une erreur peut être *informative* pour le modèle.
Si un tool échoue, le modèle peut tenter une autre approche, reformuler sa requête ou demander plus d'informations à l'utilisateur.

[source,java]
----
@Override
public McpSchema.CallToolResult call(Map<String, Object> arguments) {
    String articleId = (String) arguments.get("article_id");

    Optional<Article> article = repository.findById(articleId);

    if (article.isEmpty()) {
        // Au lieu d'une exception, on retourne une information
        return new McpSchema.CallToolResult(
            List.of(new McpSchema.TextContent(
                "Aucun article trouvé avec l'ID : " + articleId + ". " +
                "L'article a peut-être été supprimé ou l'ID est incorrect."
            )),
            true // isError = true
        );
    }

    return new McpSchema.CallToolResult(
        List.of(new McpSchema.TextContent(article.get().toJson())),
        false
    );
}
----

Le modèle peut interpréter ce message et suggérer à l'utilisateur de chercher l'article autrement.

=== Le rôle du développeur évolue

Avec les API REST, nous sommes des *intégrateurs* : nous connaissons les endpoints, écrivons le code d'orchestration, gérons les erreurs explicitement.

Avec MCP, nous devenons des *architectes d'outils* : nous concevons des tools bien délimités, rédigeons des descriptions claires et laissons le modèle orchestrer leur utilisation selon le contexte conversationnel.

Cette évolution ne rend pas les API obsolètes.
Elle introduit une nouvelle couche d'abstraction où les LLM deviennent les nouveaux clients, capables de raisonnement et d'adaptation contextuelle.

== Conclusion

Le *Model Context Protocol* n'est pas une alternative à *OpenAPI* ou aux *API* d'entreprise.
Il représente un *nouveau niveau d'abstraction* dans l'ingénierie logicielle, conçu spécifiquement pour l'ère des LLM.
Là où OpenAPI structure la communication entre systèmes avec des contrats statiques et des endpoints prédéfinis, MCP structure la collaboration entre modèles et outils avec des capacités découvertes dynamiquement et des descriptions sémantiques.
Là où une API expose des ressources manipulées via des verbes HTTP, un serveur MCP expose des intentions exécutées par raisonnement contextuel.

Cette différence de paradigme ne signifie pas l'obsolescence des API d'entreprise.
Les deux approches sont *complémentaires* et coexistent naturellement dans les architectures modernes.
Les API REST continueront d'exceller pour l'intégration système-à-système, les transactions critiques et les clients traditionnels.
MCP, quant à lui, résout le problème spécifique de l'accès contextuel aux outils pour les LLM.
Un serveur MCP peut d'ailleurs s'appuyer sur des API REST internes pour récupérer des données, puis exposer ces capacités sous forme de tools que les LLM peuvent invoquer intelligemment.





















<1> Indique que cette classe est une entité JPA, c’est-à-dire une table persistée en base de données.
<2> Définit la clé primaire de l’entité.
<3> Spécifie la stratégie de génération de la clé primaire, ici `IDENTITY` pour une auto-incrémentation.
<4> Définit la relation `@ManyToMany` avec un chargement paresseux `LAZY`
<5> Définit la table de jointure `user_roles` entre `User` et `Role`, avec les colonnes de jointure appropriées `user_id` et `role_id` contenant les clés étrangères.
<6> Ensemble de rôles associés à l'utilisateur, initialisé avec un HashSet pour éviter les doublons.

Nos entités `User` et `Role` sont définies avec une relation `@ManyToMany` unidirectionnelle, seul un `User` connaît ses `roles`.

NOTE: Si vous voulez avoir une relation bidirectionnelle, il faudra ajouter une relation inverse, un champ `private Set<User> users = new HashSet<>()` avec `@ManyToMany(mappedBy = "roles")` dans l'entité `Role`.
Cette relation inverse n'est pas obligatoire, mais elle peut être utile pour des opérations de navigation dans les deux sens et aussi ramener son lot de complexité dans la gestion des données.

===== Repository

[source, java]
----
public interface UserRepository extends JpaRepository<User, Long> {

    @Query("SELECT u FROM User u JOIN FETCH u.roles WHERE u.email = :email") <1>
    Optional<User> findByEmailWithRoles(@Param("email") String email); <2>
}
----

<1> Utilise une requête JPQL avec `JOIN FETCH` pour charger l'utilisateur et ses rôles en une seule requête.
<2> Définit une méthode de recherche par email qui retourne un utilisateur avec ses rôles chargés.

===== Utilisation

[source, java]
----
var user = userRepository.findByEmailWithRoles("user@example.com")
                          .orElseThrow(() -> new UserNotFoundException("Utilisateur non trouvé"));
// Accès aux rôles sans risque de LazyInitializationException
var roles = user.getRoles();
----

==== Exemple de code - Quarkus avec Panache

Voici ce qui change avec Panache par rapport à l'exemple JPA classique:

- Modèle Active Record: les entités étendent `PanacheEntity`, héritent d'un identifiant et exposent des méthodes de persistance et de requête directement sur la classe (`User.find(...)`).
- Champs publics pour la concision; les getters/setters sont optionnels mais peuvent être ajoutés si besoin (validation, encapsulation).
- Pas d'interface Repository Spring Data: soit on interroge l'entité directement, soit on encapsule dans un service CDI (`@ApplicationScoped`) ou un repository Panache (implémente `PanacheRepository<T>`) pour structurer la logique d'accès.
- Requêtes dynamiques avec `find`, `list`, `stream`, etc.

===== Entité

[source, java]
----
@Entity
public class User extends PanacheEntity {
    public String email;

    @ManyToMany
    @JoinTable(
        name = "user_roles",
        joinColumns = @JoinColumn(name = "user_id"),
        inverseJoinColumns = @JoinColumn(name = "role_id")
    )
    public Set<Role> roles = new HashSet<>();
}

@Entity
public class Role extends PanacheEntity {
    public String name;
}
----

===== Service

[source, java]
----
@ApplicationScoped <1>
public class UserService {

    public Optional<User> findByEmailWithRoles(String email) {
        return User.find("SELECT u FROM User u JOIN FETCH u.roles WHERE u.email = ?1", email)
                   .firstResultOptional();
    }
}
----

<1> Indique que cette classe est un bean CDI avec une seule instance pour toute l'application et non un bean instancié à chaque injection.

===== Utilisation

[source, java]
----
var user = userService.findByEmailWithRoles("user@example.com")
                       .orElseThrow(() -> new UserNotFoundException("Utilisateur non trouvé"));
// Accès aux rôles sans risque de LazyInitializationException
var roles = user.getRoles();
----

Dans les deux exemples :

- La requête JPQL utilise `JOIN FETCH` pour charger immédiatement les rôles associés à l’utilisateur dans une seule requête.
- Cela évite les requêtes N+1 et les exceptions `LazyInitializationException` dans les contextes transactionnels courts.

NOTE: Un **contexte transactionnel court** désigne une période d'exécution pendant laquelle une transaction est ouverte pour accomplir une tâche ciblée comme une lecture, une mise à jour ou une suppression et se termine rapidement par un commit ou un rollback.

Avant d’utiliser `JOIN FETCH`, il est essentiel d’évaluer la cardinalité et le volume de données de la relation.
Réservez-le de préférence aux associations simples et à cardinalité unique (`@ManyToOne`, `@OneToOne`), et privilégiez pour les collections (`@OneToMany`, `@ManyToMany`) une requête dédiée ou un `EntityGraph` pour un chargement plus précis et maîtrisé.

IMPORTANT: Utilisez `LEFT JOIN FETCH` si la relation est optionnelle (`nullable = true`), afin de conserver les entités principales même lorsqu’aucune association n’est présente.

==== Bonnes pratiques et anti-patterns

Voici quelques bonnes pratiques et anti-patterns à connaître lors de l'utilisation de JOIN FETCH :

**Anti-pattern : Multiplication cartésienne**

[source, java]
----
// Anti-pattern avec JOIN FETCH - Risque de multiplication cartésienne
@Query("SELECT u FROM User u JOIN FETCH u.roles JOIN FETCH u.permissions")
List<User> findAllWithRolesAndPermissions(); // Problématique avec de grands volumes
----

Cette requête peut générer une explosion cartésienne des résultats si un utilisateur a plusieurs rôles ET plusieurs permissions, chaque combinaison étant retournée comme une ligne distincte.

**Bonne pratique : Utiliser DISTINCT ou des requêtes séparées**

[source, java]
----
// Bonne pratique - Utiliser DISTINCT pour éviter les doublons
@Query("SELECT DISTINCT u FROM User u JOIN FETCH u.roles")
List<User> findAllWithRolesDistinct();

// Alternative - Séparer les requêtes pour les relations multiples
@Query("SELECT u FROM User u WHERE u.id = :id")
Optional<User> findById(@Param("id") Long id);

@Query("SELECT r FROM Role r JOIN r.users u WHERE u.id = :userId")
List<Role> findRolesByUserId(@Param("userId") Long userId);
----

== Entity Graph – Une approche déclarative et modulaire

Introduits avec **JPA 2.1**, les **Entity Graphs** offrent une alternative déclarative et découplée à `JOIN FETCH`, mieux adaptée aux architectures modulaires et évolutives.
Ils permettent de spécifier explicitement les associations à charger, sans modifier la requête **JPQL** elle-même, ce qui réduit le couplage entre la logique métier et la stratégie de récupération.

Un **Entity Graph** se définit au niveau de l'entité elle-même de façon statique via l’annotation `@NamedEntityGraph`, ou dynamiquement à l’exécution en utilisant l'API de l'`EntityManager`.
Cette approche favorise une séparation claire des responsabilités, en externalisant les choix de chargement, tout en maintenant un code propre, réutilisable et plus facile à tester.

=== Cas d'utilisation typique

Charger un utilisateur avec ses rôles de manière déclarative, sans intégrer la stratégie de chargement directement dans la requête JPQL.
Cela permet de centraliser la configuration des associations.
Le même `Entity Graph` peut ainsi être réutilisé dans différents contextes fonctionnels tels que l’affichage des informations utilisateur, les contrôles d’accès (sécurité) ou les interfaces d’administration.

==== Exemple de code - Spring Boot avec JPA Repository

Ce qui caractérise l'exemple Spring Data JPA avec Entity Graph:

- Déclaration du graphe au niveau de l'entité via `@NamedEntityGraph` pour une configuration réutilisable et centralisée.
- Activation au niveau du repository avec `@EntityGraph`, soit par nom (`value = "User.withRoles"`), soit ad hoc (`attributePaths = {"roles"}`).
- Le paramètre `type` pilote la stratégie de chargement.
- Pas de JPQL nécessaire: on conserve des méthodes idiomatiques (`findByEmail`, `findAll`) tout en contrôlant précisément le chargement.

===== Entité

[source, java]
----
@Entity
@NamedEntityGraph(
    name = "User.withRoles", <1>
    attributeNodes = @NamedAttributeNode("roles") <2>
)
public class User {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    private String email;

    @ManyToMany(fetch = FetchType.LAZY)
    @JoinTable(
        name = "user_roles",
        joinColumns = @JoinColumn(name = "user_id"),
        inverseJoinColumns = @JoinColumn(name = "role_id")
    )
    private Set<Role> roles = new HashSet<>();

    // Getters / Setters
}

@Entity
public class Role {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    private String name;

    // Getters / Setters
}
----

<1> Définit un Entity Graph nommé `User.withRoles` au niveau de l'entité `User`.
<2> Spécifie que l'attribut `roles` doit être chargé lorsque cet Entity Graph est utilisé.

===== Repository

Spring Data JPA fournit une intégration native des Entity Graphs via l’annotation `@EntityGraph`.
Cela permet d’associer un graphe à une méthode de repository sans écrire de JPQL.

[source, java]
----
public interface UserRepository extends JpaRepository<User, Long> {

    @EntityGraph(value = "User.withRoles", type = EntityGraph.EntityGraphType.FETCH) <1>
    Optional<User> findByEmail(String email);

    @EntityGraph(attributePaths = {"roles"}) <2>
    List<User> findAll();
}
----

<1> Utilise l'Entity Graph nommé défini au niveau de l'entité `User`.
<2> Définit un Entity Graph ad hoc directement dans la méthode (utilisation dynamique de l'Entity Graph).

NOTE: Le paramètre **type** de l'annotation `@EntityGraph` permet de spécifier le type de chargement (`FETCH` ou `LOAD`), que nous allons voir plus en détail dans la section suivante.

===== Utilisation

[source, java]
----
// Utilisation de l'Entity Graph nommé
var user = userRepository.findByEmail("user@example.com")
                         .orElseThrow(() -> new UserNotFoundException("Utilisateur non trouvé"));
// Accès aux rôles sans risque de LazyInitializationException
var roles = user.getRoles();

// Utilisation de l'Entity Graph ad-hoc
var allUsers = userRepository.findAll();
// Tous les utilisateurs ont leurs rôles chargés
----

==== Exemple de code - Quarkus avec Panache

Voici ce qui change avec Panache pour l'usage des Entity Graphs par rapport à l'exemple Spring Data JPA:

- Pas d'interface Repository Spring Data: on travaille soit en Active Record directement sur l'entité (méthodes Panache), soit via un service CDI (`@ApplicationScoped`), soit via un repository Panache (implémente `PanacheRepository&lt;T&gt;`) pour structurer la logique d'accès.
- Graphes nommés vs dynamiques: on déclare des graphes avec `@NamedEntityGraph` sur l'entité ou on les construit à l'exécution (`entityManager.createEntityGraph(...)`).
- Activation côté Quarkus/Panache: on applique le graphe via les hints JPA (`jakarta.persistence.fetchgraph` ou `jakarta.persistence.loadgraph`) soit avec l'`EntityManager#setHint(...)`, soit via l'API Panache avec `withHint(...)` sur les requêtes (`find(...)`, `findAll()`, etc.).

===== Entité

[source, java]
----
@Entity
@NamedEntityGraph(
    name = "User.withRoles",
    attributeNodes = @NamedAttributeNode("roles")
)
public class User extends PanacheEntity {
    public String email;

    @ManyToMany
    @JoinTable(
        name = "user_roles",
        joinColumns = @JoinColumn(name = "user_id"),
        inverseJoinColumns = @JoinColumn(name = "role_id")
    )
    public Set<Role> roles = new HashSet<>();
}

@Entity
public class Role extends PanacheEntity {
    public String name;
}
----

===== Repository / Service

[source, java]
----
@ApplicationScoped
public class UserService {

    @Inject <1>
    EntityManager entityManager;

    // Méthode 1: Utilisation de l'Entity Graph nommé
    public Optional<User> findByEmailWithRoles(String email) {
        return entityManager.createQuery("SELECT u FROM User u WHERE u.email = :email", User.class)
                .setParameter("email", email)
                .setHint("jakarta.persistence.fetchgraph",
                         entityManager.getEntityGraph("User.withRoles"))
                .getResultStream()
                .findFirst();
    }

    // Méthode 2: Création dynamique d'un Entity Graph
    public List<User> findAllWithRoles() {
        EntityGraph<?> graph = entityManager.createEntityGraph(User.class);
        graph.addAttributeNodes("roles");

        return entityManager.createQuery("SELECT u FROM User u", User.class)
                .setHint("jakarta.persistence.fetchgraph", graph)
                .getResultList();
    }
}
----

<1> Injecte l'`EntityManager` pour accéder aux fonctionnalités JPA.

===== Exemple de cas complexe - Entity Graph avec relations imbriquées

Dans des scénarios plus complexes, vous pourriez avoir besoin de charger non seulement les rôles d'un utilisateur, mais aussi d'autres relations imbriquées comme le département auquel il appartient et le manager de ce département. Voici comment définir un Entity Graph plus sophistiqué :

[source, java]
----
@Entity
@NamedEntityGraphs({
    @NamedEntityGraph(
        name = "User.withRoles",
        attributeNodes = @NamedAttributeNode("roles")
    ),
    @NamedEntityGraph(
        name = "User.withRolesAndDepartment",
        attributeNodes = {
            @NamedAttributeNode("roles"),
            @NamedAttributeNode(value = "department", subgraph = "departmentGraph")
        },
        subgraphs = {
            @NamedSubgraph(
                name = "departmentGraph",
                attributeNodes = @NamedAttributeNode("manager")
            )
        }
    )
})
public class User {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    private String email;

    @ManyToMany(fetch = FetchType.LAZY)
    private Set<Role> roles = new HashSet<>();

    @ManyToOne(fetch = FetchType.LAZY)
    private Department department;

    // Getters / Setters
}

@Entity
public class Department {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    private String name;

    @ManyToOne(fetch = FetchType.LAZY)
    private User manager;

    // Getters / Setters
}
----

Avec ce graphe nommé `User.withRolesAndDepartment`, vous pouvez charger en une seule requête :

. L'utilisateur
. Ses rôles
. Son département
. Le manager du département

Utilisation dans le repository :

[source, java]
----
@EntityGraph(value = "User.withRolesAndDepartment", type = EntityGraph.EntityGraphType.FETCH)
Optional<User> findByEmail(String email);
----

Cette approche est particulièrement utile pour les écrans de détail ou les rapports qui nécessitent des données provenant de plusieurs entités liées.

===== Le hint `jakarta.persistence.fetchgraph`

Le hint `jakarta.persistence.fetchgraph` est un paramètre clé dans l'API JPA qui permet de contrôler précisément le chargement des associations lors de l'exécution d'une requête.
Contrairement au chargement `EAGER` global ou aux requêtes `JOIN FETCH`, ce hint offre une approche plus flexible et contextuelle :

. *Fonctionnement* (fetchgraph) : il remplace temporairement toutes les stratégies de chargement définies sur l'entité pour la requête courante.
   - Les attributs spécifiés dans l'Entity Graph sont chargés **EAGER** (immédiatement)
   - Les attributs non spécifiés sont chargés **LAZY** (à la demande)
   - Cela s'applique uniquement à la requête courante, sans modifier la configuration de l'entité

. *Différence avec `jakarta.persistence.loadgraph*` :
   - `fetchgraph` : seuls les attributs spécifiés sont chargés **EAGER**, tous les autres deviennent **LAZY**
   - `loadgraph` : les attributs spécifiés sont chargés **EAGER**, les autres conservent leur configuration d'origine (EAGER ou LAZY)

. *Avantages* :
   - Contrôle précis du chargement **sans modifier les entités**
   - Réduction des problèmes de performance liés au sur-chargement
   - Séparation claire entre la **logique de requête** et la **stratégie de chargement**

===== Extension Panache

[source, java]
----
// Extension de PanacheRepository pour ajouter le support des Entity Graphs
@ApplicationScoped
public class UserRepository implements PanacheRepository<User> {

    @Inject
    EntityManager em;

    // Méthode utilisant un Entity Graph
    public Optional<User> findByEmailWithRoles(String email) {
        // Obtenir l'Entity Graph nommé
        EntityGraph<?> graph = em.getEntityGraph("User.withRoles");

        // Utiliser l'Entity Graph avec une requête Panache
        return find("email", email)
                .withHint("jakarta.persistence.fetchgraph", graph)
                .firstResultOptional();
    }

    // Méthode avec Entity Graph dynamique
    public List<User> listAllWithRoles() {
        // Créer un Entity Graph dynamique
        EntityGraph<User> graph = em.createEntityGraph(User.class);
        graph.addAttributeNodes("roles");

        // Appliquer l'Entity Graph à la requête
        return findAll()
                .withHint("jakarta.persistence.fetchgraph", graph)
                .list();
    }
}
----

===== Utilisation

[source, java]
----
@Inject
UserService userService;

// Utilisation de l'Entity Graph nommé
var user = userService.findByEmailWithRoles("user@example.com")
                      .orElseThrow(() -> new NotFoundException("Utilisateur non trouvé"));
// Accès aux rôles sans risque de LazyInitializationException
var roles = user.roles;

// Utilisation de l'Entity Graph dynamique
var allUsers = userService.findAllWithRoles();
// Tous les utilisateurs ont leurs rôles chargés
----

== Criteria API – Une approche dynamique et typée

Introduite avec **JPA 2.0**, la **Criteria API** offre une alternative programmatique aux requêtes JPQL statiques.
Elle permet de construire dynamiquement des requêtes typées et sécurisées à l'exécution, sans recourir à des concaténations de chaînes de caractères.

Cette approche est particulièrement adaptée aux scénarios où les critères de recherche sont variables et déterminés par l'utilisateur,
comme dans les interfaces de recherche avancée ou les tableaux de bord personnalisables.

IMPORTANT: Privilégiez la Criteria API lorsque vous devez **construire des requêtes dynamiques** basées sur des conditions définies à l'exécution.
Elle excelle dans les cas de filtres multi-critères, de tri dynamique, de pagination ou de jointures conditionnelles.
Contrairement à JOIN FETCH ou Entity Graph, elle s'adresse aux situations où la structure de la requête ne peut être connue à l'avance.

=== Cas d'utilisation typique

Prenons un scénario classique avec un moteur de recherche utilisateur, filtré sur des attributs facultatifs **nom**, **rôle**, **date de création**, etc.
Une approche JPQL nécessiterait une explosion de méthodes ; avec Criteria, on peut composer dynamiquement :

[source, java]
----
CriteriaBuilder cb = em.getCriteriaBuilder();
CriteriaQuery<User> cq = cb.createQuery(User.class);
Root<User> user = cq.from(User.class);
List<Predicate> predicates = new ArrayList<>();

if (filter.getName() != null) {
    predicates.add(cb.like(user.get("name"), "%" + filter.getName() + "%"));
}
if (filter.getRole() != null) {
    Join<User, Role> roles = user.join("roles");
    predicates.add(cb.equal(roles.get("name"), filter.getRole()));
}

cq.select(user).where(predicates.toArray(new Predicate[0]));
return em.createQuery(cq).getResultList();
----

Cette flexibilité est particulièrement utile dans les interfaces où les critères sont choisis par l'utilisateur, ou dans les systèmes embarquant des moteurs de filtrage complexes.

NOTE: La Criteria API est un excellent choix pour les systèmes à logique d’interrogation conditionnelle, comme les backoffices, les interfaces d’administration ou les API exposant des options de tri et de recherche.

==== Une puissance qui a un coût

Mais cette expressivité s'accompagne d'un niveau de verbosité important.
Le code devient rapidement technique, parfois difficile à lire ou à maintenir.
La logique métier se retrouve noyée dans une syntaxe typée souvent déroutante pour les développeurs moins expérimentés.
Là où une requête JPQL prendrait trois lignes, une construction Criteria peut en nécessiter dix à quinze, avec peu de gain de lisibilité.

Par ailleurs, la réutilisabilité reste limitée : chaque nouvelle construction nécessite de reprendre les blocs de construction et les assembler à nouveau,
mais l'effort de conception reste plus élevé qu'avec un EntityGraph ou une requête JPQL bien ciblée.

NOTE: Il existe des surcouches comme **QueryDSL**, **JPA Specifications** ou **Blaze-Persistence**, qui proposent une écriture plus concise ou plus expressive,
tout en conservant la puissance du modèle Criteria.

==== Intégration avec Spring (JPA Specification)

Spring propose une surcouche très pratique via le pattern `Specification<T>`, qui encapsule la construction Criteria de manière réutilisable et testable :

[source, java]
----
public class UserSpecifications {
    public static Specification<User> hasName(String name) {
        return (root, query, cb) ->
            cb.like(root.get("name"), "%" + name + "%");
    }

    public static Specification<User> hasRole(String roleName) {
        return (root, query, cb) -> {
            Join<User, Role> roles = root.join("roles");
            return cb.equal(roles.get("name"), roleName);
        };
    }
}

----

Appel combiné dans le repository

[source, java]
----
userRepository.findAll(
    Specification.where(hasName("jhon")).and(hasRole("ADMIN"))
);
----

==== Exemple avancé - Recherche avec filtrage et tri dynamiques

Pour illustrer la puissance de la Criteria API dans des scénarios réels, voici un exemple plus complet de recherche utilisateur avec filtrage multi-critères, tri dynamique et pagination :

[source, java]
----
// Classe de critères de recherche
public class UserSearchCriteria {
    private String email;
    private String roleName;
    private LocalDate createdAfter;
    private String sortBy = "email";
    private boolean ascending = true;
    private int page = 0;
    private int size = 20;

    // Getters et setters
}

// Service de recherche
@Service
public class UserSearchService {

    @PersistenceContext
    private EntityManager em;

    public Page<User> searchUsers(UserSearchCriteria criteria) {
        CriteriaBuilder cb = em.getCriteriaBuilder();

        // Requête pour les données
        CriteriaQuery<User> query = cb.createQuery(User.class);
        Root<User> root = query.from(User.class);

        // Requête pour le count total
        CriteriaQuery<Long> countQuery = cb.createQuery(Long.class);
        Root<User> countRoot = countQuery.from(User.class);

        // Prédicats pour le filtrage
        List<Predicate> predicates = buildPredicates(cb, root, criteria);
        List<Predicate> countPredicates = buildPredicates(cb, countRoot, criteria);

        // Application des prédicats
        if (!predicates.isEmpty()) {
            query.where(predicates.toArray(new Predicate[0]));
            countQuery.where(countPredicates.toArray(new Predicate[0]));
        }

        // Tri dynamique
        applySort(cb, query, root, criteria);

        // Exécution des requêtes
        countQuery.select(cb.count(countRoot));
        Long total = em.createQuery(countQuery).getSingleResult();

        // Pagination
        List<User> users = em.createQuery(query)
                .setFirstResult(criteria.getPage() * criteria.getSize())
                .setMaxResults(criteria.getSize())
                .getResultList();

        return new PageImpl<>(users, PageRequest.of(
                criteria.getPage(), criteria.getSize(),
                Sort.by(criteria.isAscending() ? Sort.Direction.ASC : Sort.Direction.DESC,
                        criteria.getSortBy())),
                total);
    }

    private List<Predicate> buildPredicates(CriteriaBuilder cb, Root<User> root, UserSearchCriteria criteria) {
        List<Predicate> predicates = new ArrayList<>();

        // Filtrage par email
        if (criteria.getEmail() != null && !criteria.getEmail().isEmpty()) {
            predicates.add(cb.like(cb.lower(root.get("email")),
                    "%" + criteria.getEmail().toLowerCase() + "%"));
        }

        // Filtrage par rôle
        if (criteria.getRoleName() != null && !criteria.getRoleName().isEmpty()) {
            Join<User, Role> roleJoin = root.join("roles", JoinType.LEFT);
            predicates.add(cb.equal(roleJoin.get("name"), criteria.getRoleName()));

            // Éviter les doublons si jointure sur collection
            query.distinct(true);
        }

        // Filtrage par date de création
        if (criteria.getCreatedAfter() != null) {
            predicates.add(cb.greaterThanOrEqualTo(
                    root.get("createdAt"), criteria.getCreatedAfter()));
        }

        return predicates;
    }

    private void applySort(CriteriaBuilder cb, CriteriaQuery<User> query,
                          Root<User> root, UserSearchCriteria criteria) {
        // Tri dynamique selon le champ spécifié
        String sortField = criteria.getSortBy();
        boolean ascending = criteria.isAscending();

        // Validation du champ de tri (sécurité)
        if (!isValidSortField(sortField)) {
            sortField = "email"; // Valeur par défaut sécurisée
        }

        // Application du tri
        if (ascending) {
            query.orderBy(cb.asc(root.get(sortField)));
        } else {
            query.orderBy(cb.desc(root.get(sortField)));
        }
    }

    private boolean isValidSortField(String field) {
        // Liste blanche des champs de tri autorisés
        return Arrays.asList("email", "id", "createdAt").contains(field);
    }
}
----

Cet exemple illustre plusieurs aspects avancés de la Criteria API :

. **Filtrage multi-critères** : application conditionnelle de plusieurs prédicats
. **Jointures dynamiques** : ajout de jointures uniquement si nécessaire
. **Pagination optimisée** : requête distincte pour le count total
. **Tri dynamique sécurisé** : validation des champs de tri pour éviter les injections
. **Retour paginé** : utilisation de l'API `Page` de Spring pour une pagination complète

Cette approche est particulièrement adaptée aux interfaces de recherche avancée où l'utilisateur peut sélectionner librement ses critères de filtrage et de tri.

==== Integration avec Quarkus

Dans Quarkus, l’approche Criteria API est pleinement supportée via Hibernate ORM. On retrouve l’usage classique :

[source, java]
----
// Exemple d'utilisation de Criteria API avec Quarkus
@ApplicationScoped
public class UserService {

    @Inject
    EntityManager em;

    // Méthode de recherche avec critères dynamiques
    public List<User> searchUsers(String email, String roleName) {
        // Création du CriteriaBuilder et de la requête
        CriteriaBuilder cb = em.getCriteriaBuilder();
        CriteriaQuery<User> query = cb.createQuery(User.class);
        Root<User> user = query.from(User.class);

        // Liste pour stocker les prédicats
        List<Predicate> predicates = new ArrayList<>();

        // Ajout conditionnel des critères
        if (email != null && !email.isEmpty()) {
            predicates.add(cb.like(user.get("email"), "%" + email + "%"));
        }

        if (roleName != null && !roleName.isEmpty()) {
            // Jointure avec les rôles
            Join<User, Role> roleJoin = user.join("roles");
            predicates.add(cb.equal(roleJoin.get("name"), roleName));
            query.distinct(true); // Évite les doublons
        }

        // Application des prédicats à la requête
        if (!predicates.isEmpty()) {
            query.where(predicates.toArray(new Predicate[0]));
        }

        // Tri par email
        query.orderBy(cb.asc(user.get("email")));

        // Exécution de la requête avec pagination
        return em.createQuery(query)
                .setMaxResults(20)
                .getResultList();
    }

    // Exemple d'utilisation
    public List<User> findAdmins() {
        return searchUsers("jhone@exemple.com", "ADMIN");
    }
}
----

===== Approche inspirée des Specifications avec Panache

Quarkus Panache permet également d'implémenter un pattern similaire aux Specifications de Spring, offrant une approche plus modulaire et réutilisable pour construire des requêtes dynamiques.

[source, java]
----
// Classe utilitaire pour les critères de recherche d'utilisateurs
public class UserCriteria {

    // Interface fonctionnelle pour définir un critère
    @FunctionalInterface
    public interface Criterion {
        void apply(CriteriaBuilder cb, CriteriaQuery<?> query, Root<User> root, List<Predicate> predicates);

        // Méthodes par défaut pour combiner les critères
        default Criterion and(Criterion other) {
            return (cb, query, root, predicates) -> {
                this.apply(cb, query, root, predicates);
                other.apply(cb, query, root, predicates);
            };
        }

        default Criterion or(Criterion other) {
            return (cb, query, root, predicates) -> {
                List<Predicate> thisPredicates = new ArrayList<>();
                List<Predicate> otherPredicates = new ArrayList<>();

                this.apply(cb, query, root, thisPredicates);
                other.apply(cb, query, root, otherPredicates);

                if (!thisPredicates.isEmpty() && !otherPredicates.isEmpty()) {
                    predicates.add(cb.or(
                        cb.and(thisPredicates.toArray(new Predicate[0])),
                        cb.and(otherPredicates.toArray(new Predicate[0]))
                    ));
                } else if (!thisPredicates.isEmpty()) {
                    predicates.addAll(thisPredicates);
                } else if (!otherPredicates.isEmpty()) {
                    predicates.addAll(otherPredicates);
                }
            };
        }
    }

    // Critères réutilisables
    public static Criterion hasEmail(String email) {
        return (cb, query, root, predicates) -> {
            if (email != null && !email.isEmpty()) {
                predicates.add(cb.like(root.get("email"), "%" + email + "%"));
            }
        };
    }

    public static Criterion hasRole(String roleName) {
        return (cb, query, root, predicates) -> {
            if (roleName != null && !roleName.isEmpty()) {
                Join<User, Role> roleJoin = root.join("roles");
                predicates.add(cb.equal(roleJoin.get("name"), roleName));
                query.distinct(true); // Évite les doublons
            }
        };
    }
}

// Repository Panache avec support des critères
@ApplicationScoped
public class UserRepository implements PanacheRepository<User> {

    @Inject
    EntityManager em;

    // Méthode générique pour appliquer des critères
    public List<User> findByCriteria(UserCriteria.Criterion criterion) {
        CriteriaBuilder cb = em.getCriteriaBuilder();
        CriteriaQuery<User> query = cb.createQuery(User.class);
        Root<User> root = query.from(User.class);

        List<Predicate> predicates = new ArrayList<>();

        // Application du critère
        if (criterion != null) {
            criterion.apply(cb, query, root, predicates);
        }

        // Construction de la requête
        if (!predicates.isEmpty()) {
            query.where(predicates.toArray(new Predicate[0]));
        }

        // Tri par défaut
        query.orderBy(cb.asc(root.get("email")));

        // Exécution de la requête
        return em.createQuery(query).getResultList();
    }

    // Exemple d'utilisation
    public List<User> findAdmins() {
        return findByCriteria(
            UserCriteria.hasEmail("john").and(UserCriteria.hasRole("ADMIN"))
        );
    }
}
----

Cette approche offre plusieurs avantages :

. *Réutilisabilité* : Les critères sont définis une seule fois et peuvent être combinés de différentes façons.
. *Lisibilité* : L'API fluide permet d'exprimer clairement l'intention des requêtes.
. *Testabilité* : Chaque critère peut être testé individuellement.
. *Extensibilité* : De nouveaux critères peuvent être ajoutés sans modifier le code existant.

L'utilisation est similaire à celle des Specifications de Spring, mais adaptée au modèle Panache de Quarkus :

[source, java]
----
// Exemple d'utilisation dans un service
@ApplicationScoped
public class UserService {

    @Inject
    UserRepository userRepository;

    public List<User> findActiveAdmins() {
        return userRepository.findByCriteria(
            UserCriteria.hasRole("ADMIN").and(UserCriteria.hasEmail("active"))
        );
    }

    public List<User> findSupportOrSalesUsers() {
        return userRepository.findByCriteria(
            UserCriteria.hasRole("SUPPORT").or(UserCriteria.hasRole("SALES"))
        );
    }
}
----

=== Conclusion

S’il est tentant de chercher une réponse unique à la question « quelle stratégie de récupération utiliser ? », l’expérience montre qu’il n’existe pas de solution universelle en JPA. Chaque approche `JOIN FETCH`, `Entity Graph`, `Criteria API` répond à un besoin précis, avec ses forces et ses compromis.

**JOIN FETCH** offre une solution directe, efficace et prédictible, idéale dans des contextes simples ou orientés performance immédiate. Mais sa rigidité, son couplage fort avec la logique métier et sa faible réutilisabilité limitent son emploi dans des systèmes évolutifs.

**Entity Graph** propose une voie plus déclarative, modulaire et réutilisable. Elle s’inscrit naturellement dans des architectures bien structurées, où l’on cherche à séparer les préoccupations métier et infrastructure. C’est une approche particulièrement pertinente pour les projets à long cycle de vie, sensibles à la maintenabilité.

Quant à la **Criteria API**, elle devient incontournable dès que la requête dépend de critères dynamiques, choisis à l’exécution ou pilotés par l’utilisateur. Sa puissance n’a d’égale que sa complexité, et elle doit être maniée avec méthode pour ne pas compromettre la lisibilité ou la testabilité du code.

Pour choisir la stratégie la plus adaptée à votre contexte, posez-vous ces questions :

- Avez-vous besoin d'une solution simple et directe pour des cas d'utilisation bien définis ? → JOIN FETCH
- Cherchez-vous à découpler la logique métier des stratégies de chargement dans une architecture évolutive ? → Entity Graph
- Devez-vous construire des requêtes dont la structure varie selon les critères utilisateur ? → Criteria API


Quelle que soit votre plateforme de prédilection, Spring Boot ou Quarkus, ces trois approches sont pleinement supportées avec leurs spécificités propres. L'important est de comprendre les implications de chaque choix sur la performance, la maintenabilité et l'évolutivité de votre application.

En définitive, la maîtrise de ces stratégies de récupération constitue un atout majeur pour tout développeur JPA. Plutôt que de s'enfermer dans une approche unique, l'expertise consiste à savoir alterner entre ces stratégies selon les besoins spécifiques de chaque fonctionnalité, en gardant toujours à l'esprit le contexte global de l'application.
